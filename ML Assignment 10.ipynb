{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e03393",
   "metadata": {},
   "source": [
    "# 1. Define the Bayesian interpretation of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4d9d0",
   "metadata": {},
   "source": [
    "The Bayesian interpretation of probability is a perspective or framework that defines probability as a measure of uncertainty or belief in an event or hypothesis. It is based on the principles of Bayesian probability theory, which was developed by Reverend Thomas Bayes and further expanded by Pierre-Simon Laplace.\n",
    "\n",
    "According to the Bayesian interpretation, probability is subjective and reflects an individual's state of knowledge or belief about an event or hypothesis. It quantifies the degree of belief or confidence assigned to different possible outcomes. This differs from the frequentist interpretation of probability, which defines probability in terms of long-run frequencies of events.\n",
    "\n",
    "Key principles of the Bayesian interpretation include:\n",
    "\n",
    "Prior Probability: Prior probability represents the initial belief or probability assigned to an event before any evidence or data is observed. It reflects the individual's subjective knowledge or belief based on prior experiences, background information, or expert opinions.\n",
    "\n",
    "Likelihood: Likelihood refers to the probability of observing the available evidence or data given a specific hypothesis or model. It represents the conditional probability of the data given the hypothesis and is often determined using statistical or mathematical models.\n",
    "\n",
    "Posterior Probability: The posterior probability is the updated belief or probability assigned to an event or hypothesis after considering the available evidence. It is calculated using Bayes' theorem, which combines the prior probability and the likelihood to obtain the revised or updated probability.\n",
    "\n",
    "Bayes' Theorem: Bayes' theorem mathematically expresses the relationship between the prior probability, likelihood, and posterior probability. It states that the posterior probability is proportional to the product of the prior probability and the likelihood, with a normalization factor to ensure that the probabilities sum to 1.\n",
    "\n",
    "The Bayesian interpretation allows for the incorporation of prior knowledge and the iterative updating of beliefs as new evidence becomes available. It provides a coherent framework for reasoning under uncertainty and enables the quantification of uncertainty through probability distributions.\n",
    "\n",
    "Bayesian probability has applications in various fields, including statistics, machine learning, decision analysis, and artificial intelligence. It is particularly useful in scenarios with limited data, complex models, and the need for incorporating prior knowledge or expert opinions into the analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f2305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3629fd41",
   "metadata": {},
   "source": [
    "# 2. Define probability of a union of two events with equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dc8f9",
   "metadata": {},
   "source": [
    "The probability of the union of two events A and B, denoted as P(A ∪ B), is the probability that at least one of the two events occurs. It can be calculated using the following equation:\n",
    "\n",
    "P(A ∪ B) = P(A) + P(B) - P(A ∩ B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A) represents the probability of event A occurring.\n",
    "P(B) represents the probability of event B occurring.\n",
    "P(A ∩ B) represents the probability of both events A and B occurring simultaneously.\n",
    "The equation takes into account that when calculating the probability of the union of two events, the probability of their intersection (where both events occur together) is counted twice. Therefore, to avoid double counting, we subtract the probability of the intersection from the sum of the individual probabilities.\n",
    "\n",
    "By using this equation, we can compute the probability of the union of two events based on the individual probabilities of the events and their intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181801a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b3e9023",
   "metadata": {},
   "source": [
    "# 3. What is joint probability? What is its formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9dd5c",
   "metadata": {},
   "source": [
    "\n",
    "Joint probability refers to the probability of two or more events occurring simultaneously. It measures the likelihood of the intersection or overlap between events. The joint probability of two events A and B, denoted as P(A and B) or P(A, B), is calculated using the following formula:\n",
    "\n",
    "P(A and B) = P(A) × P(B|A)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A and B) represents the joint probability of events A and B occurring simultaneously.\n",
    "P(A) represents the probability of event A occurring.\n",
    "P(B|A) represents the conditional probability of event B occurring given that event A has already occurred.\n",
    "The formula calculates the joint probability by multiplying the probability of event A by the conditional probability of event B given that event A has already occurred. It takes into account the dependency or relationship between the two events.\n",
    "\n",
    "Note that if events A and B are independent, meaning that the occurrence of one event does not affect the probability of the other, the formula simplifies to:\n",
    "\n",
    "P(A and B) = P(A) × P(B)\n",
    "\n",
    "In this case, the joint probability is simply the product of the individual probabilities of the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c97c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e96f103",
   "metadata": {},
   "source": [
    "# 4. What is chain rule of probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6c179",
   "metadata": {},
   "source": [
    "The chain rule of probability, also known as the multiplication rule, is a fundamental rule in probability theory that allows us to calculate the probability of the intersection of multiple events. It provides a way to express the joint probability of multiple events in terms of conditional probabilities.\n",
    "\n",
    "The chain rule of probability states that the joint probability of n events A₁, A₂, ..., Aₙ can be calculated as the product of the conditional probabilities of each event given the preceding events:\n",
    "\n",
    "P(A₁ ∩ A₂ ∩ ... ∩ Aₙ) = P(A₁) × P(A₂|A₁) × P(A₃|A₁ ∩ A₂) × ... × P(Aₙ|A₁ ∩ A₂ ∩ ... ∩ Aₙ₋₁)\n",
    "\n",
    "In other words, to compute the joint probability of multiple events, we multiply the probability of the first event by the conditional probability of the second event given the first event, then multiply it by the conditional probability of the third event given the first two events, and so on, until we reach the conditional probability of the nth event given all preceding events.\n",
    "\n",
    "The chain rule of probability is derived from the definition of conditional probability. By applying this rule, we can break down complex joint probabilities into a series of simpler conditional probabilities. It is particularly useful in situations where the events are dependent or interconnected.\n",
    "\n",
    "Note that the chain rule can be extended to more than two events by continuing the pattern of multiplying each conditional probability given all preceding events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52df9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48491cda",
   "metadata": {},
   "source": [
    "# 5. What is conditional probability means? What is the formula of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d28cb",
   "metadata": {},
   "source": [
    "Conditional probability is a measure of the probability of an event occurring given that another event has already occurred. It quantifies the likelihood of an outcome based on the knowledge or information about a related event. In other words, it expresses the probability of an event A happening, given that event B has occurred.\n",
    "\n",
    "The conditional probability of event A given event B is denoted as P(A|B) and is calculated using the following formula:\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A occurring given that event B has occurred.\n",
    "P(A ∩ B) represents the joint probability of events A and B occurring simultaneously.\n",
    "P(B) represents the probability of event B occurring.\n",
    "The formula calculates the conditional probability by dividing the joint probability of events A and B by the probability of event B. It measures the proportion of the joint probability relative to the occurrence of event B.\n",
    "\n",
    "The conditional probability allows us to update or revise the probability of an event based on new information or evidence provided by the occurrence of another event. It is an essential concept in Bayesian probability theory and plays a crucial role in decision-making, inference, and statistical modeling.\n",
    "\n",
    "It is important to note that the formula assumes that P(B) is not equal to 0 (i.e., event B has a non-zero probability of occurring). Additionally, the formula assumes that P(A ∩ B) is the joint probability of both events A and B occurring, which may vary depending on the independence or dependence between the events.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c1d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a6888ad",
   "metadata": {},
   "source": [
    "# 6. What are continuous random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49baca75",
   "metadata": {},
   "source": [
    "Continuous random variables are variables that can take on any value within a specified range or interval. Unlike discrete random variables, which can only assume specific, separate values, continuous random variables can take on an infinite number of values within a continuous range.\n",
    "\n",
    "Some key characteristics of continuous random variables are:\n",
    "\n",
    "Infinite Range: Continuous random variables can take on values over an infinite range, such as all real numbers between -∞ and +∞. The range is typically specified based on the context of the problem or the domain of the variable.\n",
    "\n",
    "Uncountable Values: Since the values of continuous random variables are not limited to specific points, they are uncountable. This means that the number of possible values is infinitely large and cannot be listed exhaustively.\n",
    "\n",
    "Probability Density Function (PDF): Continuous random variables are described by a probability density function (PDF) rather than a probability mass function (PMF). The PDF represents the distribution of probabilities for the different values that the variable can take. It provides information about the likelihood of the variable falling within a specific range of values.\n",
    "\n",
    "Probability as Area under the Curve: In the case of continuous random variables, the probability of a specific value occurring is typically zero since the number of possible values is infinite. Instead, probabilities are calculated as areas under the probability density curve over intervals or ranges of values.\n",
    "\n",
    "Examples of continuous random variables include:\n",
    "\n",
    "Height: The height of a person can take on any value within a range, such as between 150 cm and 200 cm.\n",
    "Temperature: The temperature in a given location can have infinitely many values within a continuous range, such as between -∞ and +∞ degrees Celsius.\n",
    "Time: The time it takes for a process to complete can be any positive real number, such as the time it takes for a customer to complete a transaction.\n",
    "Continuous random variables are commonly encountered in various fields, including physics, economics, finance, and engineering. They are modeled using probability distributions such as the normal distribution, uniform distribution, exponential distribution, and many others, depending on the specific characteristics and properties of the variable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ee9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1116fa45",
   "metadata": {},
   "source": [
    "# 7. What are Bernoulli distributions? What is the formula of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea9dbe",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that models a random variable that can take only two possible outcomes: success (typically represented by the value 1) or failure (typically represented by the value 0). It is named after Jacob Bernoulli, a Swiss mathematician.\n",
    "\n",
    "The Bernoulli distribution is characterized by a single parameter, often denoted as p, which represents the probability of success. The formula for the Bernoulli distribution is as follows:\n",
    "\n",
    "P(X = x) = p^x * (1 - p)^(1 - x)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(X = x) represents the probability that the random variable X takes the value x (either 0 or 1) in a single trial.\n",
    "p represents the probability of success, i.e., the probability that X = 1.\n",
    "x is the observed value of the random variable (either 0 or 1).\n",
    "Key properties of the Bernoulli distribution include:\n",
    "\n",
    "Support: The support of the Bernoulli distribution is the set {0, 1}, representing the possible outcomes of the random variable.\n",
    "Mean: The mean or expected value of the Bernoulli distribution is given by E(X) = p.\n",
    "Variance: The variance of the Bernoulli distribution is Var(X) = p * (1 - p).\n",
    "Skewness: The skewness of the Bernoulli distribution is (1 - 2p) / sqrt(p * (1 - p)).\n",
    "Mode: The mode of the Bernoulli distribution is the value with the higher probability, which is p if p > 0.5 and 1 - p if p ≤ 0.5.\n",
    "The Bernoulli distribution is often used as a building block for more complex probability distributions, such as the binomial distribution (which models the number of successes in a fixed number of independent Bernoulli trials) and the geometric distribution (which models the number of failures before the first success in a sequence of Bernoulli trials).\n",
    "\n",
    "Applications of the Bernoulli distribution include modeling the outcome of coin flips (heads or tails), success or failure of an experiment, presence or absence of a certain characteristic, and many other binary events.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb581d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049cafae",
   "metadata": {},
   "source": [
    "# 8. What is binomial distribution? What is the formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fd505",
   "metadata": {},
   "source": [
    "The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials. It is widely used in statistics and probability theory to analyze and predict the outcomes of repeated experiments or trials with two possible outcomes: success (typically represented by the value 1) or failure (typically represented by the value 0).\n",
    "\n",
    "The binomial distribution is characterized by two parameters: the number of trials, denoted as n, and the probability of success in each trial, denoted as p. The formula for the binomial distribution is as follows:\n",
    "\n",
    "P(X = k) = C(n, k) * p^k * (1 - p)^(n - k)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(X = k) represents the probability that the random variable X takes the value k (where k ranges from 0 to n) in a series of n independent trials.\n",
    "C(n, k) represents the binomial coefficient, also known as \"n choose k,\" which calculates the number of ways to choose k successes from n trials. It can be calculated as C(n, k) = n! / (k! * (n - k)!), where \"!\" denotes the factorial.\n",
    "p represents the probability of success in each trial.\n",
    "k is the observed number of successes.\n",
    "Key properties of the binomial distribution include:\n",
    "\n",
    "Support: The support of the binomial distribution is the set of non-negative integers from 0 to n, representing the possible number of successes in the given number of trials.\n",
    "Mean: The mean or expected value of the binomial distribution is given by E(X) = n * p.\n",
    "Variance: The variance of the binomial distribution is Var(X) = n * p * (1 - p).\n",
    "Skewness: The skewness of the binomial distribution is (1 - 2p) / sqrt(n * p * (1 - p)).\n",
    "Mode: The mode of the binomial distribution is the value with the highest probability, which is the integer part of (n + 1) * p.\n",
    "The binomial distribution is widely applied in various fields, including statistics, genetics, finance, quality control, and market research. It provides a useful framework for modeling and analyzing events with binary outcomes that involve a fixed number of independent trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9beaa7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f899476",
   "metadata": {},
   "source": [
    "# 9. What is Poisson distribution? What is the formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f599970",
   "metadata": {},
   "source": [
    "The Poisson distribution is a discrete probability distribution that models the number of events that occur in a fixed interval of time or space, given a known average rate of occurrence. It is often used to describe rare events or events that occur randomly and independently over time.\n",
    "\n",
    "The Poisson distribution is characterized by a single parameter, denoted as λ (lambda), which represents the average rate of event occurrences in the given interval. The formula for the Poisson distribution is as follows:\n",
    "\n",
    "P(X = k) = (e^(-λ) * λ^k) / k!\n",
    "\n",
    "Where:\n",
    "\n",
    "P(X = k) represents the probability that the random variable X takes the value k (where k is a non-negative integer) in the specified interval.\n",
    "e is the base of the natural logarithm (approximately equal to 2.71828).\n",
    "λ represents the average rate of event occurrences in the interval.\n",
    "k is the observed number of events.\n",
    "Key properties of the Poisson distribution include:\n",
    "\n",
    "Support: The support of the Poisson distribution is the set of non-negative integers, representing the possible number of events in the specified interval.\n",
    "Mean: The mean or expected value of the Poisson distribution is given by E(X) = λ.\n",
    "Variance: The variance of the Poisson distribution is Var(X) = λ.\n",
    "Skewness: The skewness of the Poisson distribution is 1 / sqrt(λ).\n",
    "Mode: The mode of the Poisson distribution is the largest integer less than or equal to λ.\n",
    "The Poisson distribution is commonly used in various fields to model events such as the number of customer arrivals in a specific time period, the number of phone calls received per hour, the number of accidents in a given area, or the number of radioactive particles detected in a certain time frame. It is particularly applicable when the events are rare and independent, and the average rate of occurrence is known or estimated.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d29d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c145b0a3",
   "metadata": {},
   "source": [
    "# 10. Define covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5dec57",
   "metadata": {},
   "source": [
    "Covariance is a measure of how two random variables vary together. It quantifies the relationship between two variables and indicates the extent to which changes in one variable are associated with changes in another variable. Specifically, covariance measures the degree to which the variables move in the same or opposite directions.\n",
    "\n",
    "Mathematically, the covariance between two random variables X and Y is denoted as Cov(X, Y) and is calculated as follows:\n",
    "\n",
    "Cov(X, Y) = E[(X - E(X))(Y - E(Y))]\n",
    "\n",
    "Where:\n",
    "\n",
    "Cov(X, Y) represents the covariance between X and Y.\n",
    "X and Y are the random variables.\n",
    "E(X) and E(Y) denote the expected values (means) of X and Y, respectively.\n",
    "In simpler terms, the covariance is the average of the product of the deviations of X from its mean and Y from its mean. A positive covariance indicates that X and Y tend to increase or decrease together, while a negative covariance suggests that one variable tends to increase when the other decreases. A covariance of zero means that there is no linear relationship between the variables.\n",
    "\n",
    "Key properties of covariance include:\n",
    "\n",
    "Units of Measurement: Covariance has units that are the product of the units of the two variables. For example, if X is measured in meters and Y is measured in kilograms, the covariance will be in square meters-kilograms.\n",
    "\n",
    "Dependence on Scale: Covariance depends on the scale of the variables. Consequently, comparing covariances between different pairs of variables may not provide meaningful insights unless the variables are on similar scales.\n",
    "\n",
    "Symmetry: Covariance is symmetric, which means Cov(X, Y) = Cov(Y, X).\n",
    "\n",
    "Covariance is a useful tool in statistics and data analysis to understand the relationship between variables. However, it does not provide a standardized measure of association between variables since it depends on the scale of the variables. To address this limitation, the concept of correlation is often used, which is a normalized version of covariance that ranges between -1 and 1.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c32b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35852513",
   "metadata": {},
   "source": [
    "# 11. Define correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b1a00",
   "metadata": {},
   "source": [
    "Correlation is a statistical measure that quantifies the strength and direction of the relationship between two variables. It determines how closely the variables are related to each other and the extent to which they tend to change together. Correlation provides insights into the linear association between variables, indicating whether they move in the same direction, opposite directions, or have no apparent relationship.\n",
    "\n",
    "The most commonly used measure of correlation is the Pearson correlation coefficient, denoted by the symbol r. The formula for calculating the Pearson correlation coefficient between two variables X and Y is as follows:\n",
    "\n",
    "r = (Cov(X, Y)) / (σ(X) * σ(Y))\n",
    "\n",
    "Where:\n",
    "\n",
    "r represents the correlation coefficient.\n",
    "Cov(X, Y) is the covariance between X and Y.\n",
    "σ(X) and σ(Y) denote the standard deviations of X and Y, respectively.\n",
    "The Pearson correlation coefficient ranges between -1 and 1, where:\n",
    "\n",
    "A correlation coefficient of 1 indicates a perfect positive correlation, meaning the variables have a strong linear relationship and move in the same direction.\n",
    "A correlation coefficient of -1 indicates a perfect negative correlation, meaning the variables have a strong linear relationship but move in opposite directions.\n",
    "A correlation coefficient of 0 suggests no linear relationship between the variables.\n",
    "Key properties of correlation include:\n",
    "\n",
    "No Causation: Correlation does not imply causation. Even if two variables are strongly correlated, it does not necessarily mean that changes in one variable cause changes in the other. Correlation only measures the strength of the relationship, not the cause-effect nature of the relationship.\n",
    "\n",
    "Scale-Invariant: Correlation is scale-invariant, meaning it is not affected by changes in the scale or units of measurement of the variables. It solely depends on the linear relationship between the variables.\n",
    "\n",
    "Symmetry: The correlation coefficient is symmetric, indicating that the correlation between X and Y is the same as the correlation between Y and X.\n",
    "\n",
    "Correlation analysis is widely used in various fields, including statistics, economics, finance, social sciences, and machine learning. It helps identify patterns, assess the strength of relationships, select relevant variables, and make predictions based on observed data. However, it is important to note that correlation measures only linear relationships and may not capture complex nonlinear associations between variables.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d94558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99340f49",
   "metadata": {},
   "source": [
    "# 12. Define sampling with replacement. Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811adb9",
   "metadata": {},
   "source": [
    "Sampling with replacement is a method in statistics and probability theory where each selected element from a population or dataset is returned to the population before the next selection is made. In other words, after an element is chosen, it is put back into the pool of available elements, allowing it to be selected again in subsequent draws.\n",
    "\n",
    "Example:\n",
    "Let's consider a bag containing five colored balls: red, blue, green, yellow, and orange. We want to randomly select two balls from the bag using sampling with replacement.\n",
    "\n",
    "In the first draw, we randomly select a ball from the bag. Suppose we draw a blue ball.\n",
    "After noting down the color of the ball, we return the blue ball to the bag, and the bag still contains all five balls.\n",
    "In the second draw, we again randomly select a ball from the bag. This time, we may select the same blue ball again or a different one.\n",
    "We note down the color of the second ball and complete the sampling process.\n",
    "By using sampling with replacement, each ball has the same probability of being selected at each draw, regardless of previous selections. Therefore, it is possible to select the same element multiple times.\n",
    "\n",
    "Sampling with replacement is commonly used in scenarios where the population size is large or when the sampling process needs to maintain the original distribution of the population. It allows for repeated sampling of elements, making it useful for statistical inference, bootstrapping, and simulation studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ccaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2bda4ad",
   "metadata": {},
   "source": [
    "# 13. What is sampling without replacement? Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ce149",
   "metadata": {},
   "source": [
    "Sampling without replacement is a method in statistics and probability theory where each selected element from a population or dataset is not returned to the population before the next selection is made. In other words, once an element is chosen, it is removed from the pool of available elements, and subsequent selections are made from the reduced population.\n",
    "\n",
    "Example:\n",
    "Let's consider a deck of playing cards containing 52 cards. We want to randomly select three cards from the deck using sampling without replacement.\n",
    "\n",
    "In the first draw, we randomly select a card from the deck. Suppose we draw a 5 of hearts.\n",
    "After noting down the card, we remove the 5 of hearts from the deck, leaving 51 cards.\n",
    "In the second draw, we randomly select another card from the remaining deck. Let's say we draw a Queen of spades.\n",
    "Again, we remove the Queen of spades from the deck, leaving 50 cards.\n",
    "In the third and final draw, we select one more card from the remaining deck, resulting in, for example, a 9 of clubs.\n",
    "We note down all three cards and complete the sampling process.\n",
    "Sampling without replacement ensures that each element can only be selected once. As elements are removed from the population, the probability of selecting each subsequent element changes, reflecting the reduced pool of available elements.\n",
    "\n",
    "Sampling without replacement is commonly used when it is important to avoid duplicate selections or when the sampling process needs to reflect the original composition of the population. It is frequently employed in surveys, experiments, and sampling techniques where the goal is to obtain a representative sample from a larger population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b5021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34495de2",
   "metadata": {},
   "source": [
    "# 14. What is hypothesis? Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b9e4f",
   "metadata": {},
   "source": [
    "In the context of statistics and scientific research, a hypothesis is a tentative explanation or proposition that is subject to testing and evaluation. It is a statement that suggests a relationship or difference between variables or phenomena, and it serves as the basis for conducting experiments or statistical analyses to gather evidence and draw conclusions.\n",
    "\n",
    "A hypothesis typically consists of two components:\n",
    "\n",
    "Null Hypothesis (H0): This is the default assumption or claim that there is no significant relationship or difference between variables. It represents the status quo or the absence of an effect. In statistical testing, researchers aim to either reject or fail to reject the null hypothesis based on the evidence obtained.\n",
    "\n",
    "Alternative Hypothesis (Ha or H1): This is the opposite or alternative claim to the null hypothesis. It suggests that there is a significant relationship or difference between variables, and it is what researchers are typically interested in demonstrating or supporting.\n",
    "\n",
    "Example:\n",
    "Let's consider a scenario where a researcher wants to investigate the effect of a new drug on reducing blood pressure. The researcher might formulate the following null and alternative hypotheses:\n",
    "\n",
    "Null Hypothesis (H0): The new drug has no significant effect on reducing blood pressure.\n",
    "Alternative Hypothesis (Ha): The new drug has a significant effect on reducing blood pressure.\n",
    "To test these hypotheses, the researcher would conduct a study or experiment by administering the drug to a group of participants and measuring their blood pressure before and after the treatment. The collected data would then be analyzed using statistical techniques to determine whether there is sufficient evidence to support the alternative hypothesis and reject the null hypothesis.\n",
    "\n",
    "The formulation of clear and testable hypotheses is a fundamental step in the scientific method and helps guide the research process. By formulating hypotheses, researchers make predictions about the relationship between variables and provide a framework for designing experiments and collecting data to validate or invalidate their claims.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
