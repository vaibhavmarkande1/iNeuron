{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e03393",
   "metadata": {},
   "source": [
    "# 1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4d9d0",
   "metadata": {},
   "source": [
    "In Bayesian statistics, prior probability refers to the initial belief or probability assigned to an event or hypothesis before any new data or evidence is taken into account. It represents the knowledge or belief about the event based on prior information, previous experience, or subjective judgment.\n",
    "\n",
    "Prior probability is denoted as P(H), where H represents the hypothesis or event of interest. It provides a starting point for Bayesian inference, which involves updating the prior probability based on new data to obtain the posterior probability.\n",
    "\n",
    "Example:\n",
    "Let's consider a medical scenario where a patient shows certain symptoms, and the doctor wants to determine the probability of the patient having a particular disease (let's call it Disease X). The doctor has prior knowledge about the prevalence of Disease X in the general population, which can be used as the prior probability.\n",
    "\n",
    "Suppose the doctor knows from previous studies that Disease X occurs in approximately 5% of the population. In this case, the prior probability of the patient having Disease X, denoted as P(Disease X), is 0.05 or 5%.\n",
    "\n",
    "This prior probability reflects the doctor's belief about the likelihood of Disease X before considering any specific symptoms or test results for the patient. It serves as the initial probability assigned to the hypothesis (patient having Disease X) based on prior knowledge or general information about the population.\n",
    "\n",
    "Once the patient undergoes diagnostic tests or additional examinations, the doctor can update the prior probability using Bayesian inference by incorporating the new evidence. The prior probability is combined with the likelihood of the observed symptoms or test results given the disease (P(symptoms|Disease X)) to calculate the posterior probability (P(Disease X|symptoms)). The posterior probability represents the updated probability of the patient having the disease, considering both the prior information and the new data.\n",
    "\n",
    "In summary, the prior probability represents the initial belief or probability assigned to an event or hypothesis before considering any new evidence or data. It provides a starting point for Bayesian analysis and is updated using Bayes' theorem to obtain the posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f2305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3629fd41",
   "metadata": {},
   "source": [
    "# 2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dc8f9",
   "metadata": {},
   "source": [
    "In Bayesian statistics, the posterior probability refers to the updated probability of a hypothesis or event after taking into account new data or evidence. It is obtained by combining the prior probability (initial belief) with the likelihood of the observed data given the hypothesis. The posterior probability represents the revised or updated belief in light of the new information.\n",
    "\n",
    "The formula for calculating the posterior probability using Bayes' theorem is:\n",
    "\n",
    "P(H|D) = (P(D|H) * P(H)) / P(D)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(H|D) represents the posterior probability of hypothesis H given data D.\n",
    "P(D|H) is the likelihood of observing the data D given hypothesis H.\n",
    "P(H) is the prior probability of hypothesis H.\n",
    "P(D) is the marginal probability of observing the data D.\n",
    "Example:\n",
    "Let's say a diagnostic test is used to determine whether a patient has a certain medical condition (let's call it Condition X). The prior probability of the patient having Condition X, based on previous studies or general information, is 0.10 or 10%.\n",
    "\n",
    "The diagnostic test has been shown to have an accuracy rate of 90%, meaning that it correctly identifies the condition 90% of the time (P(Positive Test Result|Condition X) = 0.90). However, there is also a false positive rate of 5%, which means that the test incorrectly indicates the presence of the condition in 5% of healthy individuals (P(Positive Test Result|No Condition X) = 0.05).\n",
    "\n",
    "Suppose the patient takes the test, and the result comes out positive (Positive Test Result). We can use Bayes' theorem to calculate the posterior probability of the patient actually having Condition X (P(Condition X|Positive Test Result)).\n",
    "\n",
    "Using the formula:\n",
    "P(Condition X|Positive Test Result) = (P(Positive Test Result|Condition X) * P(Condition X)) / P(Positive Test Result)\n",
    "\n",
    "Assuming P(Positive Test Result) can be calculated using the law of total probability, incorporating both true positive and false positive cases, we can substitute the values and calculate the posterior probability.\n",
    "\n",
    "Let's assume P(Positive Test Result) = (P(Positive Test Result|Condition X) * P(Condition X)) + (P(Positive Test Result|No Condition X) * P(No Condition X))\n",
    "\n",
    "After performing the calculations, suppose we find that P(Condition X|Positive Test Result) = 0.64 or 64%. This means that, after considering the positive test result, the updated belief or posterior probability of the patient having Condition X is 64%.\n",
    "\n",
    "In summary, the posterior probability represents the revised probability of a hypothesis or event after incorporating new data or evidence. It combines the prior probability with the likelihood of the observed data given the hypothesis, using Bayes' theorem. The posterior probability reflects the updated belief or probability based on the available information.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181801a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b3e9023",
   "metadata": {},
   "source": [
    "# 3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9dd5c",
   "metadata": {},
   "source": [
    "In statistics, the likelihood is a concept used to measure the probability of observing a set of data given a specific statistical model or hypothesis. It quantifies how well the model or hypothesis explains the observed data.\n",
    "\n",
    "The likelihood is often denoted as L(θ | X), where θ represents the parameters of the statistical model and X represents the observed data. It is a function of the parameters, given the data.\n",
    "\n",
    "The likelihood function is defined as the probability density function (pdf) or probability mass function (pmf) of the data, treated as a function of the parameters while considering the data fixed. In other words, it calculates the probability of observing the given data under different parameter values.\n",
    "\n",
    "Example:\n",
    "Let's consider a simple example of flipping a fair coin. The outcome of flipping a coin can be either a \"head\" (H) or a \"tail\" (T). Suppose we have flipped the coin five times and obtained the following sequence of outcomes: H, T, H, H, T.\n",
    "\n",
    "We want to assess the likelihood of observing this particular sequence of outcomes assuming the coin is fair (i.e., the probability of getting a head or tail is 0.5).\n",
    "\n",
    "Assuming each coin flip is independent, the likelihood of the observed data can be calculated as follows:\n",
    "\n",
    "L(θ | X) = P(X | θ) = P(H, T, H, H, T | θ) = P(H | θ) * P(T | θ) * P(H | θ) * P(H | θ) * P(T | θ)\n",
    "\n",
    "Since we assume a fair coin, the probability of getting a head or tail is 0.5 (θ = 0.5). Therefore, we can substitute the probabilities and calculate the likelihood:\n",
    "\n",
    "L(θ | X) = 0.5 * 0.5 * 0.5 * 0.5 * 0.5 = 0.03125\n",
    "\n",
    "The likelihood in this case is 0.03125 or 3.125%. This represents the probability of obtaining the specific sequence of outcomes (H, T, H, H, T) under the assumption of a fair coin.\n",
    "\n",
    "The likelihood function plays a crucial role in maximum likelihood estimation (MLE), where the goal is to estimate the values of the parameters that maximize the likelihood of the observed data. The likelihood provides a measure of the fit between the data and the model, allowing researchers to evaluate different hypotheses or models based on their ability to explain the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c97c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e96f103",
   "metadata": {},
   "source": [
    "# 4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6c179",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a probabilistic machine learning algorithm used for classification tasks. It is based on Bayes' theorem, which calculates the probability of a hypothesis or class given the observed evidence. The classifier assumes that the features are conditionally independent of each other, which is where the \"naïve\" aspect of its name comes from.\n",
    "\n",
    "The term \"naïve\" in Naïve Bayes refers to the simplifying assumption of independence made by the algorithm. It assumes that all features are independent of each other given the class label, even though this may not be true in reality. This assumption allows for computational efficiency and simplifies the modeling process, making the algorithm relatively easy to implement and efficient in practice.\n",
    "\n",
    "Despite its simplifying assumptions, Naïve Bayes can perform well in many real-world classification tasks, especially in situations where the independence assumption holds reasonably well or when there is a large amount of data available. It is particularly effective when working with high-dimensional datasets where the number of features is large compared to the number of samples.\n",
    "\n",
    "The Naïve Bayes classifier calculates the posterior probability of each class given the observed features and then assigns the class label with the highest probability. It utilizes the Bayes' theorem, where the posterior probability is proportional to the product of the prior probability and the likelihood.\n",
    "\n",
    "The Naïve Bayes classifier has been successfully applied in various domains, including text classification, spam filtering, sentiment analysis, and medical diagnosis. It is known for its simplicity, efficiency, and ability to handle large-scale datasets. However, its performance may be affected if the independence assumption is violated or if there is insufficient training data to accurately estimate the probabilities. Nonetheless, Naïve Bayes remains a popular and widely used algorithm in machine learning due to its simplicity and effectiveness in many practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52df9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48491cda",
   "metadata": {},
   "source": [
    "# 5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d28cb",
   "metadata": {},
   "source": [
    "The optimal Bayes classifier, also known as the Bayes optimal classifier or Bayes optimal decision rule, is a theoretical classifier that achieves the minimum possible error rate among all possible classifiers. It is considered the \"best\" classifier in terms of classification accuracy when the true underlying data distribution and class probabilities are known.\n",
    "\n",
    "The optimal Bayes classifier makes decisions based on the posterior probabilities of each class given the observed features, using the Bayes' theorem. It assigns the sample to the class with the highest posterior probability. Mathematically, the optimal Bayes classifier can be defined as:\n",
    "\n",
    "argmax P(C | x)\n",
    "\n",
    "where C represents the class labels, x represents the observed features, and P(C | x) represents the posterior probability of class C given the observed features.\n",
    "\n",
    "The optimal Bayes classifier requires knowledge of the true class probabilities and the true conditional probability distributions given the features. However, in practice, these true probabilities are often unknown and need to be estimated from the training data.\n",
    "\n",
    "While the optimal Bayes classifier provides a theoretical benchmark for classification performance, it is not always achievable in practice. This is because the true underlying data distribution and class probabilities are typically unknown, and the estimation of these probabilities from finite training data can be challenging or subject to errors.\n",
    "\n",
    "In real-world scenarios, other classifiers, such as Naïve Bayes, logistic regression, support vector machines, or random forests, are commonly used as approximations to the optimal Bayes classifier. These classifiers aim to approximate the optimal decision boundary using available training data and make predictions based on learned patterns and relationships in the data.\n",
    "\n",
    "It is important to note that while the optimal Bayes classifier may not be attainable in practice, it serves as a useful reference for evaluating the performance of other classifiers and assessing the inherent limitations of the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c1d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a6888ad",
   "metadata": {},
   "source": [
    "# 6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49baca75",
   "metadata": {},
   "source": [
    "Two features of Bayesian learning methods are:\n",
    "\n",
    "Probabilistic Framework: Bayesian learning methods are based on a probabilistic framework, where probabilities play a central role in modeling and inference. Bayesian methods utilize probability distributions to represent uncertainty, prior beliefs, and the relationship between variables. This allows for a principled approach to modeling and reasoning under uncertainty.\n",
    "\n",
    "Incorporation of Prior Knowledge: Bayesian learning methods provide a systematic way to incorporate prior knowledge or beliefs into the learning process. Prior probabilities, representing existing knowledge or assumptions about the problem, are combined with observed data to obtain posterior probabilities. This allows for the fusion of prior knowledge and empirical evidence, enabling more informed and personalized predictions or decisions.\n",
    "\n",
    "These two features highlight the strengths of Bayesian learning methods in capturing uncertainty, leveraging prior knowledge, and providing a coherent framework for probabilistic inference. Bayesian approaches have found applications in various areas, including classification, regression, clustering, and reinforcement learning, and are particularly useful when dealing with limited data or when incorporating domain expertise is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ee9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1116fa45",
   "metadata": {},
   "source": [
    "# 7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea9dbe",
   "metadata": {},
   "source": [
    "In machine learning, consistent learners are algorithms or models that converge to the true underlying concept or function as the amount of training data increases. A consistent learner aims to produce increasingly accurate predictions or estimates as the size of the training dataset grows.\n",
    "\n",
    "The concept of consistency is closely related to the idea of convergence and asymptotic behavior. A consistent learner is designed to minimize the discrepancy between the learned model and the true model as the sample size approaches infinity.\n",
    "\n",
    "Formally, a learner is considered consistent if, given a target concept or function and a learning algorithm, the learner is able to converge to a hypothesis that matches the target concept with high probability as the number of training examples increases indefinitely.\n",
    "\n",
    "Consistency is an important property in machine learning because it provides theoretical guarantees about the performance of the learning algorithm. If a learner is consistent, it suggests that given enough data, the algorithm will eventually learn the true underlying concept and make accurate predictions or estimates.\n",
    "\n",
    "It's worth noting that the concept of consistency is often discussed in the context of supervised learning, where the goal is to learn a mapping between input features and corresponding output labels. Consistency may not be relevant or applicable to all types of learning tasks or algorithms.\n",
    "\n",
    "Consistent learners are desirable because they provide assurances that, with sufficient data, the learned model will converge to the true concept or function. This property allows practitioners and researchers to have confidence in the performance and reliability of the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb581d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049cafae",
   "metadata": {},
   "source": [
    "# 8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fd505",
   "metadata": {},
   "source": [
    "Two strengths of the Bayes classifier are:\n",
    "\n",
    "Strong Theoretical Foundation: The Bayes classifier is grounded in probability theory and Bayes' theorem, providing a solid theoretical foundation. It offers a principled approach to decision-making under uncertainty and allows for optimal classification based on the posterior probabilities of classes. The Bayes classifier is based on sound mathematical principles and is well-founded in statistical theory.\n",
    "\n",
    "Effective for Small Sample Sizes: The Bayes classifier can perform well even with limited training data. By incorporating prior knowledge or beliefs about the problem, it can make reasonable predictions or classifications even when the available data is sparse. The ability to utilize prior information allows the Bayes classifier to mitigate the impact of small sample sizes and make more informed decisions based on existing knowledge.\n",
    "\n",
    "These strengths highlight the benefits of the Bayes classifier in terms of its theoretical grounding and its ability to handle data scarcity. The Bayes classifier's principled approach to probabilistic modeling and decision-making, along with its ability to incorporate prior knowledge, make it a valuable tool in various domains, including text classification, spam filtering, medical diagnosis, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9beaa7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f899476",
   "metadata": {},
   "source": [
    "# 9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f599970",
   "metadata": {},
   "source": [
    "Two weaknesses of the Bayes classifier are:\n",
    "\n",
    "Strong Independence Assumption: The Bayes classifier assumes that the features are conditionally independent given the class label. This is known as the \"naïve\" assumption. While it simplifies the modeling process and allows for computational efficiency, it may not hold true in real-world scenarios where features are often correlated. Violation of the independence assumption can lead to suboptimal performance and inaccurate predictions.\n",
    "\n",
    "Sensitivity to Feature Distribution: The Bayes classifier's performance heavily relies on the distribution of the features. If the feature distribution deviates significantly from the assumed distribution in the model, the classifier may provide poor results. For example, if the features follow a complex non-linear distribution, the Bayes classifier, which assumes a particular distribution, may struggle to capture the underlying patterns and relationships accurately.\n",
    "\n",
    "These weaknesses indicate that the Bayes classifier's performance is highly dependent on the validity of its assumptions and the underlying distribution of the data. While the algorithm can be effective in certain scenarios, it may not always be the best choice when dealing with complex relationships or correlated features. Alternative classifiers that can handle dependencies and non-linear relationships, such as decision trees, support vector machines, or deep learning models, may be more suitable in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d29d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c145b0a3",
   "metadata": {},
   "source": [
    "# 10 i: Explain how Naïve Bayes classifier is used for Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5dec57",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is widely used for text classification tasks due to its simplicity and effectiveness. It can classify documents into predefined categories based on the words or features present in the text. Here is an overview of how the Naïve Bayes classifier is used for text classification:\n",
    "\n",
    "Data Preprocessing: The text data is typically preprocessed to convert it into a suitable format for the classifier. This may involve removing punctuation, converting text to lowercase, removing stop words (common words like \"and,\" \"the,\" etc.), and performing stemming or lemmatization to reduce words to their base form.\n",
    "\n",
    "Feature Extraction: The next step is to represent the text data as numerical features that the classifier can understand. This is often done using techniques like the bag-of-words model or TF-IDF (Term Frequency-Inverse Document Frequency). The bag-of-words model represents each document as a vector of word frequencies, while TF-IDF assigns weights to words based on their frequency in the document and their importance in the overall corpus.\n",
    "\n",
    "Training the Naïve Bayes Classifier: Once the features are extracted, the Naïve Bayes classifier is trained on a labeled training dataset. The classifier estimates the class conditional probabilities and the prior probabilities based on the training data. It calculates the likelihood of each feature given the class label and the prior probability of each class.\n",
    "\n",
    "Classification: After the classifier is trained, it can be used to classify new, unseen documents. The document is transformed into the same feature representation used during training. The Naïve Bayes classifier then calculates the posterior probabilities of the document belonging to each class based on the extracted features and the learned probabilities. The class label with the highest posterior probability is assigned to the document.\n",
    "\n",
    "Evaluation: The performance of the Naïve Bayes classifier is assessed using evaluation metrics such as accuracy, precision, recall, and F1 score. This is typically done by comparing the predicted class labels with the true class labels of a separate test dataset.\n",
    "\n",
    "One advantage of the Naïve Bayes classifier for text classification is its ability to handle high-dimensional feature spaces efficiently. Despite its assumption of feature independence, the classifier often performs well in practice due to the nature of text data. However, it's important to note that the effectiveness of the Naïve Bayes classifier for text classification depends on the characteristics of the dataset and the specific text classification problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c32b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35852513",
   "metadata": {},
   "source": [
    "# 10 ii: Explain how Naïve Bayes classifier is used for Spam filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b1a00",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a popular choice for spam filtering, as it can effectively classify emails into \"spam\" or \"non-spam\" categories based on the content of the email. Here's how the Naïve Bayes classifier is typically used for spam filtering:\n",
    "\n",
    "Dataset Preparation: A dataset of emails is collected and labeled as \"spam\" or \"non-spam\" (also called \"ham\"). The dataset is split into training and testing sets for model development and evaluation.\n",
    "\n",
    "Text Preprocessing: The email text undergoes preprocessing steps to clean and normalize the data. This includes removing punctuation, converting text to lowercase, and eliminating stop words. Additionally, features like URLs, email addresses, and special characters may be replaced or removed.\n",
    "\n",
    "Feature Extraction: The email text is transformed into numerical features using techniques like the bag-of-words model or TF-IDF. Each email is represented as a vector of word frequencies or TF-IDF weights. These features capture the presence or absence of specific words or phrases in the email.\n",
    "\n",
    "Training the Naïve Bayes Classifier: The Naïve Bayes classifier is trained on the labeled training dataset. It estimates the class conditional probabilities and prior probabilities based on the training data. The classifier calculates the likelihood of each feature (word) occurring in spam or non-spam emails, and the prior probability of each class.\n",
    "\n",
    "Classification: When a new, unseen email arrives, it goes through the same preprocessing and feature extraction steps as the training data. The Naïve Bayes classifier then calculates the posterior probabilities of the email being spam or non-spam based on the extracted features and the learned probabilities. The email is classified as \"spam\" or \"non-spam\" based on the highest posterior probability.\n",
    "\n",
    "Evaluation: The performance of the Naïve Bayes classifier is evaluated using metrics such as accuracy, precision, recall, and F1 score. The classifier's predictions are compared with the true labels of a separate test dataset to assess its effectiveness in correctly identifying spam and non-spam emails.\n",
    "\n",
    "One advantage of the Naïve Bayes classifier for spam filtering is its speed and efficiency, making it suitable for real-time email classification. It can handle large feature spaces and is robust against irrelevant or redundant features. However, its assumption of feature independence may not hold true in all cases, as there can be correlations between certain words or phrases in spam emails. Despite this limitation, Naïve Bayes classifiers have demonstrated good performance in spam filtering tasks and are widely used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d94558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99340f49",
   "metadata": {},
   "source": [
    "# 10 iii: Explain how Naïve Bayes classifier is used for Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811adb9",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier can be employed for market sentiment analysis, which involves determining the sentiment or opinion expressed in textual data related to the financial market. Here's how the Naïve Bayes classifier is typically utilized for market sentiment analysis:\n",
    "\n",
    "Dataset Preparation: A dataset of textual data, such as financial news articles, social media posts, or analyst reports, is collected. The dataset is labeled with sentiment categories, such as positive, negative, or neutral, based on the sentiment expressed in the text.\n",
    "\n",
    "Text Preprocessing: The textual data undergoes preprocessing steps to clean and prepare the text for analysis. This may include removing punctuation, converting text to lowercase, eliminating stop words, and performing stemming or lemmatization to reduce words to their base form.\n",
    "\n",
    "Feature Extraction: The text is transformed into numerical features using techniques like the bag-of-words model or TF-IDF. Each document is represented as a vector of word frequencies or TF-IDF weights. These features capture the occurrence and importance of specific words or phrases in the text.\n",
    "\n",
    "Training the Naïve Bayes Classifier: The Naïve Bayes classifier is trained on the labeled dataset. It estimates the class conditional probabilities and prior probabilities based on the training data. The classifier calculates the likelihood of each feature (word) occurring in each sentiment category and the prior probability of each sentiment category.\n",
    "\n",
    "Classification: When new textual data related to market sentiment is encountered, it goes through the same preprocessing and feature extraction steps as the training data. The Naïve Bayes classifier then calculates the posterior probabilities of the text belonging to each sentiment category based on the extracted features and the learned probabilities. The text is classified into the sentiment category with the highest posterior probability.\n",
    "\n",
    "Evaluation: The performance of the Naïve Bayes classifier is evaluated using standard evaluation metrics, such as accuracy, precision, recall, and F1 score. The classifier's predictions are compared with the true labels of a separate test dataset to assess its ability to accurately classify the sentiment expressed in the market-related text.\n",
    "\n",
    "The Naïve Bayes classifier is particularly suitable for market sentiment analysis due to its simplicity, speed, and ability to handle high-dimensional feature spaces efficiently. It can effectively capture patterns and associations between words and sentiment labels in the training data. However, it's important to note that the performance of the classifier heavily relies on the quality of the training data, the relevance of the extracted features, and the representativeness of the sentiment categories.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
